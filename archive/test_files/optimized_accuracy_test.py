#!/usr/bin/env python3
"""
ğŸ¯ OPTIMIZED ACCURACY VALIDATION TEST

This script tests the improved configuration to validate that we can achieve
realistic accuracy results before running full experiments.

Expected Results:
- CIFAR-10 + ResNet18: 80%+ accuracy (target: 85%+)
- MNIST + CNN: 95%+ accuracy (target: 98%+)
- Alzheimer + ResNet18: 90%+ accuracy (target: 95%+)
"""

import sys
import os
import torch
import time
from datetime import datetime

# Add the project root to the path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_configuration(dataset='CIFAR10', quick_test=True):
    """Test specific dataset configuration."""
    
    print(f"\nğŸ§ª TESTING {dataset} CONFIGURATION")
    print("=" * 60)
    
    # Import optimized config
    print("Loading optimized configuration...")
    
    # Override config file temporarily
    config_override = f"""
# Override config.py to use optimized settings
import sys
sys.path.insert(0, 'federated_learning/config')
from config_optimized import *

# Quick test overrides
if {quick_test}:
    GLOBAL_EPOCHS = 8          # Quick test: 8 epochs
    LOCAL_EPOCHS_ROOT = 8      # Quick test: 8 pretraining epochs
    VAE_EPOCHS = 8             # Quick test: 8 VAE epochs
    print("âš¡ Quick test mode: Reduced epochs for faster validation")
else:
    print("ğŸ Full test mode: Using optimized epochs for final results")

# Dataset-specific optimizations
DATASET = '{dataset}'
if DATASET == 'CIFAR10':
    MODEL = 'ResNet18'
    INPUT_CHANNELS = 3
    NUM_CLASSES = 10
    EXPECTED_ACCURACY = 0.80 if {quick_test} else 0.85
elif DATASET == 'MNIST':
    MODEL = 'CNN'
    INPUT_CHANNELS = 1
    NUM_CLASSES = 10
    EXPECTED_ACCURACY = 0.95 if {quick_test} else 0.98
elif DATASET == 'ALZHEIMER':
    MODEL = 'ResNet18'
    INPUT_CHANNELS = 3
    NUM_CLASSES = 4
    EXPECTED_ACCURACY = 0.90 if {quick_test} else 0.95

print(f"ğŸ¯ Target accuracy for {{DATASET}}: {{EXPECTED_ACCURACY:.1%}}")
"""
    
    # Write temporary config override
    with open('temp_config_test.py', 'w') as f:
        f.write(config_override)
    
    try:
        # Import the configuration
        exec(open('temp_config_test.py').read())
        
        # Import federated learning components
        from federated_learning.training.server import Server
        from federated_learning.training.client import Client
        from federated_learning.data.dataset_utils import load_dataset, create_client_datasets
        from federated_learning.utils.model_utils import set_random_seeds
        
        # Set random seed
        if 'RANDOM_SEED' in locals():
            set_random_seeds(RANDOM_SEED)
        
        print(f"\nğŸ“Š Configuration Summary:")
        print(f"   Dataset: {locals().get('DATASET', 'Unknown')}")
        print(f"   Model: {locals().get('MODEL', 'Unknown')}")
        print(f"   Global Epochs: {locals().get('GLOBAL_EPOCHS', 'Unknown')}")
        print(f"   Batch Size: {locals().get('BATCH_SIZE', 'Unknown')}")
        print(f"   Learning Rate: {locals().get('LR', 'Unknown')}")
        print(f"   Root Dataset Size: {locals().get('ROOT_DATASET_SIZE', 'Unknown')}")
        
        # Load dataset
        print(f"\nğŸ“‚ Loading {dataset} dataset...")
        start_time = time.time()
        
        root_dataset, test_dataset = load_dataset()
        print(f"   âœ… Dataset loaded in {time.time() - start_time:.2f}s")
        print(f"   ğŸ“Š Training samples: {len(root_dataset)}")
        print(f"   ğŸ“Š Test samples: {len(test_dataset)}")
        
        # Create data loaders
        root_loader = torch.utils.data.DataLoader(
            root_dataset, 
            batch_size=locals().get('BATCH_SIZE', 32), 
            shuffle=True, 
            num_workers=0
        )
        
        # Create server
        print(f"\nğŸ–¥ï¸ Creating server...")
        server = Server()
        server.set_datasets(root_loader, test_dataset)
        
        # Get initial accuracy
        print(f"\nğŸ“ˆ Evaluating initial model...")
        initial_accuracy = server.evaluate_model()
        print(f"   ğŸ“Š Initial accuracy: {initial_accuracy:.4f} ({initial_accuracy:.2%})")
        
        # Pre-train global model
        print(f"\nğŸ”§ Pre-training global model...")
        start_training = time.time()
        server._pretrain_global_model()
        
        # Evaluate after pretraining
        pretrained_accuracy = server.evaluate_model()
        training_time = time.time() - start_training
        
        print(f"   âœ… Pre-training completed in {training_time:.2f}s")
        print(f"   ğŸ“Š Pre-trained accuracy: {pretrained_accuracy:.4f} ({pretrained_accuracy:.2%})")
        
        # Check if we meet expectations
        expected_acc = locals().get('EXPECTED_ACCURACY', 0.8)
        
        if pretrained_accuracy >= expected_acc:
            print(f"   ğŸ‰ SUCCESS: Accuracy {pretrained_accuracy:.2%} meets target {expected_acc:.2%}")
            status = "âœ… PASSED"
        elif pretrained_accuracy >= expected_acc * 0.9:
            print(f"   âš ï¸  CLOSE: Accuracy {pretrained_accuracy:.2%} is close to target {expected_acc:.2%}")
            status = "âš ï¸ CLOSE"
        else:
            print(f"   âŒ FAILED: Accuracy {pretrained_accuracy:.2%} below target {expected_acc:.2%}")
            status = "âŒ FAILED"
        
        # Calculate improvement
        improvement = pretrained_accuracy - initial_accuracy
        print(f"   ğŸ“ˆ Improvement: {improvement:.4f} ({improvement:.2%})")
        
        return {
            'dataset': dataset,
            'initial_accuracy': initial_accuracy,
            'final_accuracy': pretrained_accuracy,
            'improvement': improvement,
            'training_time': training_time,
            'expected_accuracy': expected_acc,
            'status': status,
            'passed': pretrained_accuracy >= expected_acc
        }
        
    except Exception as e:
        print(f"âŒ ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            'dataset': dataset,
            'status': 'âŒ ERROR',
            'error': str(e),
            'passed': False
        }
    
    finally:
        # Clean up
        if os.path.exists('temp_config_test.py'):
            os.remove('temp_config_test.py')

def run_comprehensive_validation():
    """Run validation tests for all datasets."""
    
    print("ğŸš€ COMPREHENSIVE ACCURACY VALIDATION")
    print("=" * 80)
    print(f"â° Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Test datasets in order of complexity
    datasets = ['CIFAR10']  # Start with most problematic
    
    results = []
    total_start = time.time()
    
    for dataset in datasets:
        print(f"\n{'='*20} {dataset} TEST {'='*20}")
        result = test_configuration(dataset, quick_test=True)
        results.append(result)
        
        # Brief pause between tests
        time.sleep(2)
    
    total_time = time.time() - total_start
    
    # Summary report
    print(f"\nğŸ“‹ VALIDATION SUMMARY")
    print("=" * 80)
    
    passed_count = 0
    for result in results:
        dataset = result['dataset']
        status = result['status']
        
        if 'final_accuracy' in result:
            acc = result['final_accuracy']
            expected = result['expected_accuracy']
            print(f"   {dataset:12} | {status:12} | {acc:.2%} (target: {expected:.2%})")
        else:
            print(f"   {dataset:12} | {status:12}")
        
        if result.get('passed', False):
            passed_count += 1
    
    print(f"\nğŸ¯ Results: {passed_count}/{len(results)} tests passed")
    print(f"â±ï¸ Total time: {total_time:.2f} seconds")
    
    if passed_count == len(results):
        print(f"\nğŸ‰ ALL TESTS PASSED! Configuration is ready for full experiments.")
        recommendation = "âœ… Proceed with full experiments using optimized config"
    elif passed_count > 0:
        print(f"\nâš ï¸ PARTIAL SUCCESS: Some tests passed. Review failed cases.")
        recommendation = "âš ï¸ Review failures, consider parameter adjustments"
    else:
        print(f"\nâŒ ALL TESTS FAILED: Configuration needs significant adjustment.")
        recommendation = "âŒ Major configuration changes needed"
    
    print(f"\nğŸ’¡ Recommendation: {recommendation}")
    
    return results

if __name__ == "__main__":
    print("ğŸ§ª OPTIMIZED ACCURACY VALIDATION TEST")
    print(f"ğŸ¯ Objective: Validate improved training parameters")
    print(f"âš¡ Mode: Quick validation (8 epochs)")
    print(f"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Run validation
    results = run_comprehensive_validation()
    
    print(f"\nâœ… Validation completed!")
    print(f"ğŸ’¡ Next steps:")
    print(f"   1. If tests pass â†’ Run full experiments with main.py")
    print(f"   2. If tests fail â†’ Adjust config_optimized.py parameters")
    print(f"   3. Document final results in paper tables") 